#!/usr/bin/env python3
"""
Script de prueba para el sistema de web scraping de Auditel
"""
import sys
import os

# Agregar el directorio actual al path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers.scraper_manager import ScraperManager
from utils.cache_manager import CacheManager
from utils.text_processor import TextProcessor
import logging

# Configurar logging simple
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)

logger = logging.getLogger(__name__)


def test_cache_manager():
    """Prueba el gestor de cach√©"""
    print("\n" + "="*80)
    print("PRUEBA 1: Cache Manager")
    print("="*80)

    cache = CacheManager(cache_dir="cache_data_test")

    # Guardar datos
    datos_prueba = {"mensaje": "Hola mundo", "numero": 42}
    cache.guardar("test_key", datos_prueba)
    print("‚úÖ Datos guardados en cach√©")

    # Obtener datos
    datos_obtenidos = cache.obtener("test_key")
    if datos_obtenidos == datos_prueba:
        print("‚úÖ Datos recuperados correctamente del cach√©")
    else:
        print("‚ùå Error recuperando datos del cach√©")

    # Estad√≠sticas
    stats = cache.estadisticas()
    print(f"üìä Estad√≠sticas: {stats}")

    # Limpiar
    cache.limpiar_todo()
    print("‚úÖ Cach√© limpiado")


def test_text_processor():
    """Prueba el procesador de texto"""
    print("\n" + "="*80)
    print("PRUEBA 2: Text Processor")
    print("="*80)

    processor = TextProcessor()

    # Limpiar HTML
    html = "<p>Este es un <b>texto</b> con <script>alert('x')</script> HTML</p>"
    limpio = processor.sanitizar_html(html)
    print(f"HTML original: {html}")
    print(f"Texto limpio: {limpio}")
    print("‚úÖ Limpieza de HTML funcional")

    # Extraer keywords
    texto = "La Ley de Obras P√∫blicas regula la construcci√≥n y contrataci√≥n de infraestructura p√∫blica en M√©xico"
    keywords = processor.extraer_keywords(texto, max_keywords=5)
    print(f"\nTexto: {texto}")
    print(f"Keywords: {keywords}")
    print("‚úÖ Extracci√≥n de keywords funcional")

    # Extraer referencias legales
    texto_legal = "Conforme al Art√≠culo 123 de la Ley de Obras P√∫blicas y el Reglamento de Construcci√≥n..."
    referencias = processor.extraer_referencias_legales(texto_legal)
    print(f"\nTexto legal: {texto_legal}")
    print(f"Referencias: {referencias}")
    print("‚úÖ Extracci√≥n de referencias funcional")


def test_scrapers():
    """Prueba los scrapers"""
    print("\n" + "="*80)
    print("PRUEBA 3: Web Scrapers")
    print("="*80)

    cache = CacheManager()
    manager = ScraperManager(cache_manager=cache)

    # Consulta de prueba
    query = "obras p√∫blicas licitaci√≥n"

    print(f"\nüîç Buscando: '{query}'")
    print("‚ö†Ô∏è NOTA: Esta prueba real puede tardar 30-60 segundos...")
    print("‚ö†Ô∏è Los resultados dependen de la disponibilidad de los sitios web")

    try:
        # Buscar en todas las fuentes
        resultado = manager.buscar_en_todos(
            query=query,
            max_resultados_por_fuente=2,
            usar_cache=True
        )

        print(f"\nüìä Resultados:")
        print(f"   ‚Ä¢ Total encontrado: {resultado.total_encontrado}")
        print(f"   ‚Ä¢ Fuentes consultadas: {resultado.fuentes_consultadas}")
        print(f"   ‚Ä¢ Tiempo de b√∫squeda: {resultado.tiempo_busqueda:.2f}s")
        print(f"   ‚Ä¢ Desde cach√©: {resultado.desde_cache}")

        if resultado.normativas:
            print(f"\nüìÑ Primeras 3 normativas encontradas:")
            for i, norm in enumerate(resultado.normativas[:3], 1):
                print(f"\n   {i}. {norm.titulo[:80]}...")
                print(f"      Fuente: {norm.fuente}")
                print(f"      Tipo: {norm.tipo}")
                if norm.url:
                    print(f"      URL: {norm.url[:60]}...")

            print("\n‚úÖ Web scraping funcional!")
        else:
            print("\n‚ö†Ô∏è No se encontraron resultados (puede ser normal si los sitios cambiaron)")
            print("   El scraper est√° funcional pero los sitios pueden requerir ajustes")

    except Exception as e:
        print(f"\n‚ùå Error durante el scraping: {e}")
        print("   Esto puede ser normal si hay problemas de conectividad")

    finally:
        # Limpiar
        manager.cerrar_todos()
        print("\n‚úÖ Sesiones de scrapers cerradas")


def main():
    """Ejecuta todas las pruebas"""
    print("\n" + "#"*80)
    print("#" + " "*78 + "#")
    print("#" + " "*20 + "AUDITEL - PRUEBAS DEL SISTEMA" + " "*29 + "#")
    print("#" + " "*78 + "#")
    print("#"*80)

    try:
        # Prueba 1: Cache
        test_cache_manager()

        # Prueba 2: Text Processor
        test_text_processor()

        # Prueba 3: Scrapers (opcional, puede ser lenta)
        respuesta = input("\n¬øDeseas probar el web scraping real? (puede tardar 30-60s) [s/N]: ")
        if respuesta.lower() in ['s', 'si', 's√≠', 'y', 'yes']:
            test_scrapers()
        else:
            print("\n‚è≠Ô∏è Prueba de web scraping omitida")

        print("\n" + "="*80)
        print("‚úÖ TODAS LAS PRUEBAS COMPLETADAS")
        print("="*80)
        print("\nüí° El sistema est√° listo para usarse!")
        print("   Para iniciar la aplicaci√≥n, ejecuta: python app_v2.py")

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Pruebas interrumpidas por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error durante las pruebas: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
